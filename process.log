nohup: ignoring input
/root/BoredDoomGuy/env/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/root/BoredDoomGuy/env/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
wandb: Currently logged in as: mjsong2021 (minjunes) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run nwb3yfzq
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /root/BoredDoomGuy/wandb/run-20251021_041345-nwb3yfzq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-vortex-124
wandb: ‚≠êÔ∏è View project at https://wandb.ai/minjunes/doom-idm-curiosity
wandb: üöÄ View run at https://wandb.ai/minjunes/doom-idm-curiosity/runs/nwb3yfzq
/root/BoredDoomGuy/icm_rnd.py:482: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)
  Image.fromarray(rgb_curr, mode='RGB').save(f'{save_frame_dir}/frame_{i:04d}_current.png')
Using device: cpu
Using 30 worker threads
Warming up observation normalization with random policy...
Warmup complete.

=== Iteration 1/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 16.08s
EPOCH 1 took 17.19s
update_step :  1
reward/intrinsic_batch_mean :  0.04997724694427517
reward/extrinsic_batch_mean :  0.0034444444444444444
loss/policy :  -0.00010937998189492095
loss/rnd :  0.001214133212642716
loss/value :  0.2256508581340313
loss/value_i :  0.043436126529493115
loss/value_e :  0.182214732197198
loss/entropy :  2.483675176447088
reward/intrinsic_running :  0.46861202197275176
reward/extrinsic_running :  0.0034444444444444444
reward/intrinsic_std_running :  0.10285961878441786
reward/extrinsic_std_running :  0.19275581046789697
reward/intrinsic_batch_std :  0.007471860180611264
reward/intrinsic_batch_max :  0.13935458660125732
reward/intrinsic_batch_min :  0.038968220353126526
reward/total_batch :  0.04997724694427517
time/iteration_time :  87.62667489051819
time/fps :  3081.2535148382754
data/episodes_collected :  60
data/frames_collected :  270000
Timer 87.6s | FPS: 3081
Policy Loss: -0.0001, Value Loss: 0.2257, Entropy: 2.4837
RND Loss: 0.0012
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.486, max=1.355, sum=32796.8
Extrinsic raw: Œº=0.0034444444444444444

=== Iteration 2/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.19s
EPOCH 1 took 19.13s
update_step :  2
reward/intrinsic_batch_mean :  0.001350550444993725
reward/extrinsic_batch_mean :  0.0023777777777777777
loss/policy :  -0.00036565105564835847
loss/rnd :  0.0002523627307241303
loss/value :  0.132266592798811
loss/value_i :  0.0031368154750177355
loss/value_e :  0.12912977735201517
loss/entropy :  2.483232765486746
reward/intrinsic_running :  0.2484592759606831
reward/extrinsic_running :  0.0023777777777777777
reward/intrinsic_std_running :  0.23213732317193567
reward/extrinsic_std_running :  0.15890217660055841
reward/intrinsic_batch_std :  0.0012288705065236283
reward/intrinsic_batch_max :  0.039680156856775284
reward/intrinsic_batch_min :  0.0004130800371058285
reward/total_batch :  0.001350550444993725
time/iteration_time :  108.47071480751038
time/fps :  2489.151108473248
data/episodes_collected :  60
data/frames_collected :  270000
Timer 108.5s | FPS: 2489
Policy Loss: -0.0004, Value Loss: 0.1323, Entropy: 2.4832
RND Loss: 0.0003
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.006, max=0.171, sum=392.7
Extrinsic raw: Œº=0.0023777777777777777

=== Iteration 3/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 17.67s
EPOCH 1 took 18.34s
update_step :  3
reward/intrinsic_batch_mean :  0.0009284984457543706
reward/extrinsic_batch_mean :  0.003822222222222222
loss/policy :  -0.0008105775824250569
loss/rnd :  0.00018204722084302568
loss/value :  0.20527934576525833
loss/value_i :  0.00270784044264336
loss/value_e :  0.20257150562423648
loss/entropy :  2.482785134604483
reward/intrinsic_running :  0.17181454740946933
reward/extrinsic_running :  0.003822222222222222
reward/intrinsic_std_running :  0.21843586684906274
reward/extrinsic_std_running :  0.20018561263530613
reward/intrinsic_batch_std :  0.0008660804898545089
reward/intrinsic_batch_max :  0.02133001573383808
reward/intrinsic_batch_min :  0.00019736964895855635
reward/total_batch :  0.0009284984457543706
time/iteration_time :  108.82305359840393
time/fps :  2481.0919292560634
data/episodes_collected :  60
data/frames_collected :  270000
Timer 108.8s | FPS: 2481
Policy Loss: -0.0008, Value Loss: 0.2053, Entropy: 2.4828
RND Loss: 0.0002
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.004, max=0.098, sum=286.9
Extrinsic raw: Œº=0.003822222222222222

=== Iteration 4/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 17.92s
EPOCH 1 took 18.35s
update_step :  4
reward/intrinsic_batch_mean :  0.0008899306528136582
reward/extrinsic_batch_mean :  0.002340740740740741
loss/policy :  -0.0007702752557110437
loss/rnd :  0.00016290641735505426
loss/value :  0.1278789627055327
loss/value_i :  0.001650094218679111
loss/value_e :  0.12622886828400873
loss/entropy :  2.4811247767824116
reward/intrinsic_running :  0.13292115917702324
reward/extrinsic_running :  0.002340740740740741
reward/intrinsic_std_running :  0.2008722720254341
reward/extrinsic_std_running :  0.15896098580110377
reward/intrinsic_batch_std :  0.0007916444594521874
reward/intrinsic_batch_max :  0.013295983895659447
reward/intrinsic_batch_min :  0.00015449471538886428
reward/total_batch :  0.0008899306528136582
time/iteration_time :  111.32315874099731
time/fps :  2425.3713517793517
data/episodes_collected :  60
data/frames_collected :  270000
Timer 111.3s | FPS: 2425
Policy Loss: -0.0008, Value Loss: 0.1279, Entropy: 2.4811
RND Loss: 0.0002
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.004, max=0.066, sum=299.0
Extrinsic raw: Œº=0.002340740740740741

=== Iteration 5/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.75s
EPOCH 1 took 18.32s
update_step :  5
reward/intrinsic_batch_mean :  0.0008522133852264637
reward/extrinsic_batch_mean :  0.0025703703703703704
loss/policy :  -0.0002795639348650266
loss/rnd :  0.00014821438416027266
loss/value :  0.14824012057347732
loss/value_i :  0.0011529451708845568
loss/value_e :  0.14708717528617743
loss/entropy :  2.479990070516413
reward/intrinsic_running :  0.1092254610278591
reward/extrinsic_running :  0.0025703703703703704
reward/intrinsic_std_running :  0.1858442165971176
reward/extrinsic_std_running :  0.16811830229493005
reward/intrinsic_batch_std :  0.0007232234580383539
reward/intrinsic_batch_max :  0.010025111958384514
reward/intrinsic_batch_min :  0.00014695877325721085
reward/total_batch :  0.0008522133852264637
time/iteration_time :  112.66270732879639
time/fps :  2396.53392326201
data/episodes_collected :  60
data/frames_collected :  270000
Timer 112.7s | FPS: 2397
Policy Loss: -0.0003, Value Loss: 0.1482, Entropy: 2.4800
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.005, max=0.054, sum=309.5
Extrinsic raw: Œº=0.0025703703703703704

=== Iteration 6/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.10s
EPOCH 1 took 18.78s
update_step :  6
reward/intrinsic_batch_mean :  0.0008367379412512054
reward/extrinsic_batch_mean :  0.003185185185185185
loss/policy :  -0.0002840972143034874
loss/rnd :  0.00013849436667938292
loss/value :  0.20326210705168318
loss/value_i :  0.0006998819241716499
loss/value_e :  0.20256222502300233
loss/entropy :  2.478158221100316
reward/intrinsic_running :  0.09323289378529462
reward/extrinsic_running :  0.003185185185185185
reward/intrinsic_std_running :  0.17340955941963515
reward/extrinsic_std_running :  0.18486492306687402
reward/intrinsic_batch_std :  0.0006859422461109771
reward/intrinsic_batch_max :  0.008386388421058655
reward/intrinsic_batch_min :  0.00013246742310002446
reward/total_batch :  0.0008367379412512054
time/iteration_time :  113.02645349502563
time/fps :  2388.8213037833916
data/episodes_collected :  60
data/frames_collected :  270000
Timer 113.0s | FPS: 2389
Policy Loss: -0.0003, Value Loss: 0.2033, Entropy: 2.4782
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.005, max=0.048, sum=325.7
Extrinsic raw: Œº=0.003185185185185185

=== Iteration 7/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.32s
EPOCH 1 took 18.52s
update_step :  7
reward/intrinsic_batch_mean :  0.0007694468075823453
reward/extrinsic_batch_mean :  0.003125925925925926
loss/policy :  -0.0001977526398232757
loss/rnd :  0.00012231696591973588
loss/value :  0.1961912162827723
loss/value_i :  0.0005851440236614453
loss/value_e :  0.19560607251795856
loss/entropy :  2.4785644141110508
reward/intrinsic_running :  0.08154687843647607
reward/extrinsic_running :  0.003125925925925926
reward/intrinsic_std_running :  0.1630883836043861
reward/extrinsic_std_running :  0.18494605538351022
reward/intrinsic_batch_std :  0.0005074731283416758
reward/intrinsic_batch_max :  0.008068163879215717
reward/intrinsic_batch_min :  0.00012924635666422546
reward/total_batch :  0.0007694468075823453
time/iteration_time :  114.17789673805237
time/fps :  2364.730895502793
data/episodes_collected :  60
data/frames_collected :  270000
Timer 114.2s | FPS: 2365
Policy Loss: -0.0002, Value Loss: 0.1962, Entropy: 2.4786
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.005, max=0.049, sum=318.5
Extrinsic raw: Œº=0.003125925925925926

=== Iteration 8/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 19.52s
EPOCH 1 took 19.06s
update_step :  8
reward/intrinsic_batch_mean :  0.0007868003491561265
reward/extrinsic_batch_mean :  0.0022444444444444443
loss/policy :  -0.00011564944747299182
loss/rnd :  0.00011769043571389788
loss/value :  0.15483388722394453
loss/value_i :  0.0003971499783360879
loss/value_e :  0.15443673763762822
loss/entropy :  2.4798947753328267
reward/intrinsic_running :  0.0727385482417811
reward/extrinsic_running :  0.0022444444444444443
reward/intrinsic_std_running :  0.154336840817678
reward/extrinsic_std_running :  0.15911374765040898
reward/intrinsic_batch_std :  0.0005843997002595065
reward/intrinsic_batch_max :  0.008888788521289825
reward/intrinsic_batch_min :  0.00011081426782766357
reward/total_batch :  0.0007868003491561265
time/iteration_time :  116.03628635406494
time/fps :  2326.858334436359
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.0s | FPS: 2327
Policy Loss: -0.0001, Value Loss: 0.1548, Entropy: 2.4799
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.005, max=0.058, sum=344.1
Extrinsic raw: Œº=0.0022444444444444443

=== Iteration 9/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 19.11s
EPOCH 1 took 18.39s
update_step :  9
reward/intrinsic_batch_mean :  0.0008137995538202598
reward/extrinsic_batch_mean :  0.0030296296296296295
loss/policy :  -0.0004995940012811458
loss/rnd :  0.00011671739279832268
loss/value :  0.21496149175094836
loss/value_i :  0.00034908124684024546
loss/value_e :  0.21461241060134137
loss/entropy :  2.480913530696522
reward/intrinsic_running :  0.06585855143399767
reward/extrinsic_running :  0.0030296296296296295
reward/intrinsic_std_running :  0.1468142940757875
reward/extrinsic_std_running :  0.1850777790748263
reward/intrinsic_batch_std :  0.0005325169957722373
reward/intrinsic_batch_max :  0.0072950683534145355
reward/intrinsic_batch_min :  0.00012030161451548338
reward/total_batch :  0.0008137995538202598
time/iteration_time :  114.89871215820312
time/fps :  2349.8957902003212
data/episodes_collected :  60
data/frames_collected :  270000
Timer 114.9s | FPS: 2350
Policy Loss: -0.0005, Value Loss: 0.2150, Entropy: 2.4809
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.006, max=0.050, sum=374.2
Extrinsic raw: Œº=0.0030296296296296295

=== Iteration 10/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.26s
EPOCH 1 took 18.72s
update_step :  10
reward/intrinsic_batch_mean :  0.0008296396023203636
reward/extrinsic_batch_mean :  0.0034518518518518518
loss/policy :  -0.0003065038394417956
loss/rnd :  0.00011361599767538296
loss/value :  0.20669415254484524
loss/value_i :  0.00028302059769030717
loss/value_e :  0.2064111330969767
loss/entropy :  2.4818786816163496
reward/intrinsic_running :  0.06034019296262502
reward/extrinsic_running :  0.0034518518518518518
reward/intrinsic_std_running :  0.14027316328381945
reward/extrinsic_std_running :  0.19274607047033623
reward/intrinsic_batch_std :  0.0006301189592887544
reward/intrinsic_batch_max :  0.008186664432287216
reward/intrinsic_batch_min :  0.00010900636698352173
reward/total_batch :  0.0008296396023203636
time/iteration_time :  115.54814648628235
time/fps :  2336.6882828540556
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.5s | FPS: 2337
Policy Loss: -0.0003, Value Loss: 0.2067, Entropy: 2.4819
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.006, max=0.058, sum=399.2
Extrinsic raw: Œº=0.0034518518518518518

=== Iteration 11/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 19.17s
EPOCH 1 took 18.27s
update_step :  11
reward/intrinsic_batch_mean :  0.0008484283781884221
reward/extrinsic_batch_mean :  0.002562962962962963
loss/policy :  -0.0005155592800954103
loss/rnd :  0.00011156787811408073
loss/value :  0.19316072276595866
loss/value_i :  0.00024413071580718017
loss/value_e :  0.19291659275239165
loss/entropy :  2.4806946407664907
reward/intrinsic_running :  0.055806079312042525
reward/extrinsic_running :  0.002562962962962963
reward/intrinsic_std_running :  0.13452049279694775
reward/extrinsic_std_running :  0.16812943018675988
reward/intrinsic_batch_std :  0.0006109801784513071
reward/intrinsic_batch_max :  0.008001650683581829
reward/intrinsic_batch_min :  9.94372385321185e-05
reward/total_batch :  0.0008484283781884221
time/iteration_time :  115.4913318157196
time/fps :  2337.837790552261
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.5s | FPS: 2338
Policy Loss: -0.0005, Value Loss: 0.1932, Entropy: 2.4807
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.006, max=0.059, sum=425.7
Extrinsic raw: Œº=0.002562962962962963

=== Iteration 12/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.43s
EPOCH 1 took 18.96s
update_step :  12
reward/intrinsic_batch_mean :  0.0008724614643647026
reward/extrinsic_batch_mean :  0.0047777777777777775
loss/policy :  -0.0010608048944711547
loss/rnd :  0.00011047917966304037
loss/value :  0.3074364924069607
loss/value_i :  0.000222818971616851
loss/value_e :  0.30721367353742773
loss/entropy :  2.478193824941462
reward/intrinsic_running :  0.05201259589796601
reward/extrinsic_running :  0.0047777777777777775
reward/intrinsic_std_running :  0.12941497391364246
reward/extrinsic_std_running :  0.224671255036122
reward/intrinsic_batch_std :  0.0006211342035394879
reward/intrinsic_batch_max :  0.007948399521410465
reward/intrinsic_batch_min :  0.00010780505544971675
reward/total_batch :  0.0008724614643647026
time/iteration_time :  115.27650094032288
time/fps :  2342.194617268748
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.3s | FPS: 2342
Policy Loss: -0.0011, Value Loss: 0.3074, Entropy: 2.4782
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.007, max=0.061, sum=455.1
Extrinsic raw: Œº=0.0047777777777777775

=== Iteration 13/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.44s
EPOCH 1 took 18.85s
update_step :  13
reward/intrinsic_batch_mean :  0.0009596723372094049
reward/extrinsic_batch_mean :  0.004718518518518519
loss/policy :  -0.0006848927754838245
loss/rnd :  0.00011660978083399999
loss/value :  0.2875924085577329
loss/value_i :  0.00020261481426352861
loss/value_e :  0.2873897938565774
loss/entropy :  2.4772965691306372
reward/intrinsic_running :  0.04884663775349283
reward/extrinsic_running :  0.004718518518518519
reward/intrinsic_std_running :  0.1248338754424015
reward/extrinsic_std_running :  0.22473843732797474
reward/intrinsic_batch_std :  0.0007361052867891598
reward/intrinsic_batch_max :  0.008197324350476265
reward/intrinsic_batch_min :  0.00011591075599426404
reward/total_batch :  0.0009596723372094049
time/iteration_time :  115.62928509712219
time/fps :  2335.0485975349147
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.6s | FPS: 2335
Policy Loss: -0.0007, Value Loss: 0.2876, Entropy: 2.4773
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.008, max=0.066, sum=518.9
Extrinsic raw: Œº=0.004718518518518519

=== Iteration 14/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.92s
EPOCH 1 took 18.71s
update_step :  14
reward/intrinsic_batch_mean :  0.0009706011188607793
reward/extrinsic_batch_mean :  0.003311111111111111
loss/policy :  -0.0005826266067081385
loss/rnd :  0.00011447969529509658
loss/value :  0.23398810083215887
loss/value_i :  0.00019451761600617883
loss/value_e :  0.23379358333168607
loss/entropy :  2.4776609160683374
reward/intrinsic_running :  0.04612026033177003
reward/extrinsic_running :  0.003311111111111111
reward/intrinsic_std_running :  0.12070420971010455
reward/extrinsic_std_running :  0.18846396518390326
reward/intrinsic_batch_std :  0.0007299430161571979
reward/intrinsic_batch_max :  0.007757898420095444
reward/intrinsic_batch_min :  0.00010717249824665487
reward/total_batch :  0.0009706011188607793
time/iteration_time :  116.10080432891846
time/fps :  2325.5652840705447
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.1s | FPS: 2326
Policy Loss: -0.0006, Value Loss: 0.2340, Entropy: 2.4777
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.008, max=0.064, sum=542.8
Extrinsic raw: Œº=0.003311111111111111

=== Iteration 15/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.45s
EPOCH 1 took 18.80s
update_step :  15
reward/intrinsic_batch_mean :  0.0009589524405512579
reward/extrinsic_batch_mean :  0.0035703703703703705
loss/policy :  -0.0012439280169818437
loss/rnd :  0.00010983024571253213
loss/value :  0.25939011709256604
loss/value_i :  0.00018533010513761616
loss/value_e :  0.2592047859321941
loss/entropy :  2.4756807269472065
reward/intrinsic_running :  0.043725216606267825
reward/extrinsic_running :  0.0035703703703703705
reward/intrinsic_std_running :  0.11696255681928247
reward/extrinsic_std_running :  0.19658716060485146
reward/intrinsic_batch_std :  0.0006972829810396236
reward/intrinsic_batch_max :  0.007141169160604477
reward/intrinsic_batch_min :  0.0001072057566489093
reward/total_batch :  0.0009589524405512579
time/iteration_time :  116.10937476158142
time/fps :  2325.393626091063
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.1s | FPS: 2325
Policy Loss: -0.0012, Value Loss: 0.2594, Entropy: 2.4757
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.008, max=0.061, sum=553.4
Extrinsic raw: Œº=0.0035703703703703705

=== Iteration 16/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.95s
EPOCH 1 took 19.29s
update_step :  16
reward/intrinsic_batch_mean :  0.001029610016432815
reward/extrinsic_batch_mean :  0.003748148148148148
loss/policy :  -0.0011271179651940297
loss/rnd :  0.0001139287382874031
loss/value :  0.25232472530368605
loss/value_i :  0.00019458309675842017
loss/value_e :  0.2521301424413016
loss/entropy :  2.470387112010609
reward/intrinsic_running :  0.04166076850121278
reward/extrinsic_running :  0.003748148148148148
reward/intrinsic_std_running :  0.11354158334092355
reward/extrinsic_std_running :  0.20027949798066072
reward/intrinsic_batch_std :  0.0008184052785122437
reward/intrinsic_batch_max :  0.008269298821687698
reward/intrinsic_batch_min :  9.805179433897138e-05
reward/total_batch :  0.001029610016432815
time/iteration_time :  114.50850653648376
time/fps :  2357.903427148225
data/episodes_collected :  60
data/frames_collected :  270000
Timer 114.5s | FPS: 2358
Policy Loss: -0.0011, Value Loss: 0.2523, Entropy: 2.4704
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.009, max=0.073, sum=612.1
Extrinsic raw: Œº=0.003748148148148148

=== Iteration 17/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 20.76s
EPOCH 1 took 21.02s
update_step :  17
reward/intrinsic_batch_mean :  0.0010662342329306907
reward/extrinsic_batch_mean :  0.003385185185185185
loss/policy :  -0.0010043016799444917
loss/rnd :  0.00011502681158934578
loss/value :  0.2305246155821916
loss/value_i :  0.00020330806331582028
loss/value_e :  0.2303213072997151
loss/entropy :  2.4667006875529434
reward/intrinsic_running :  0.03984585238306491
reward/extrinsic_running :  0.003385185185185185
reward/intrinsic_std_running :  0.11040046485564468
reward/extrinsic_std_running :  0.18875720188115405
reward/intrinsic_batch_std :  0.0008092566556616668
reward/intrinsic_batch_max :  0.007933098822832108
reward/intrinsic_batch_min :  9.67509113252163e-05
reward/total_batch :  0.0010662342329306907
time/iteration_time :  120.49672770500183
time/fps :  2240.724749480415
data/episodes_collected :  60
data/frames_collected :  270000
Timer 120.5s | FPS: 2241
Policy Loss: -0.0010, Value Loss: 0.2305, Entropy: 2.4667
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.010, max=0.072, sum=651.9
Extrinsic raw: Œº=0.003385185185185185

=== Iteration 18/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.33s
EPOCH 1 took 18.55s
update_step :  18
reward/intrinsic_batch_mean :  0.001071558477181329
reward/extrinsic_batch_mean :  0.002777777777777778
loss/policy :  -0.0017254251958520124
loss/rnd :  0.00011267534710262252
loss/value :  0.2056337675136147
loss/value_i :  0.00016597318211676213
loss/value_e :  0.20546779454205977
loss/entropy :  2.4634607849699077
reward/intrinsic_running :  0.03821612988063081
reward/extrinsic_running :  0.002777777777777778
reward/intrinsic_std_running :  0.10750917851074995
reward/extrinsic_std_running :  0.1768334293293976
reward/intrinsic_batch_std :  0.0008270009309745698
reward/intrinsic_batch_max :  0.010320588015019894
reward/intrinsic_batch_min :  0.00010594190825941041
reward/total_batch :  0.001071558477181329
time/iteration_time :  119.79105830192566
time/fps :  2253.924490085749
data/episodes_collected :  60
data/frames_collected :  270000
Timer 119.8s | FPS: 2254
Policy Loss: -0.0017, Value Loss: 0.2056, Entropy: 2.4635
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.010, max=0.096, sum=672.8
Extrinsic raw: Œº=0.002777777777777778

=== Iteration 19/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 19.38s
EPOCH 1 took 18.67s
update_step :  19
reward/intrinsic_batch_mean :  0.0010716914268713918
reward/extrinsic_batch_mean :  0.003896296296296296
loss/policy :  -0.0011163629594992733
loss/rnd :  0.00010973947097557935
loss/value :  0.27808580773346353
loss/value_i :  0.00017808704543505993
loss/value_e :  0.2779077212467338
loss/entropy :  2.4588553616494844
reward/intrinsic_running :  0.036747187878134226
reward/extrinsic_running :  0.003896296296296296
reward/intrinsic_std_running :  0.10483640946187009
reward/extrinsic_std_running :  0.20394172276064274
reward/intrinsic_batch_std :  0.000833310770106681
reward/intrinsic_batch_max :  0.010564549826085567
reward/intrinsic_batch_min :  0.0001087539640138857
reward/total_batch :  0.0010716914268713918
time/iteration_time :  116.47062540054321
time/fps :  2318.1810784604986
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.5s | FPS: 2318
Policy Loss: -0.0011, Value Loss: 0.2781, Entropy: 2.4589
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.010, max=0.101, sum=690.0
Extrinsic raw: Œº=0.003896296296296296

=== Iteration 20/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 19.25s
EPOCH 1 took 19.12s
update_step :  20
reward/intrinsic_batch_mean :  0.001238037665728458
reward/extrinsic_batch_mean :  0.0015407407407407407
loss/policy :  -0.0010302940610555115
loss/rnd :  0.00012278072840258986
loss/value :  0.13213454965840687
loss/value_i :  0.0002593737014897422
loss/value_e :  0.1318751760391575
loss/entropy :  2.449256673003688
reward/intrinsic_running :  0.0354784207532573
reward/extrinsic_running :  0.0015407407407407407
reward/intrinsic_std_running :  0.10234419880825192
reward/extrinsic_std_running :  0.1337681666700494
reward/intrinsic_batch_std :  0.001020481045141008
reward/intrinsic_batch_max :  0.01014398317784071
reward/intrinsic_batch_min :  9.607365791453049e-05
reward/total_batch :  0.001238037665728458
time/iteration_time :  116.61680889129639
time/fps :  2315.275152586955
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.6s | FPS: 2315
Policy Loss: -0.0010, Value Loss: 0.1321, Entropy: 2.4493
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.012, max=0.099, sum=816.5
Extrinsic raw: Œº=0.0015407407407407407

=== Iteration 21/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.86s
EPOCH 1 took 19.45s
update_step :  21
reward/intrinsic_batch_mean :  0.0012861981435235874
reward/extrinsic_batch_mean :  0.0014592592592592593
loss/policy :  -0.0010368238506612904
loss/rnd :  0.00012428730251761436
loss/value :  0.1172057602441672
loss/value_i :  0.00021014112920084065
loss/value_e :  0.11699561910195784
loss/entropy :  2.446393786054669
reward/intrinsic_running :  0.03434172210591782
reward/extrinsic_running :  0.0014592592592592593
reward/intrinsic_std_running :  0.10001968945798015
reward/extrinsic_std_running :  0.1279818426336164
reward/intrinsic_batch_std :  0.0010346917902682923
reward/intrinsic_batch_max :  0.009199073538184166
reward/intrinsic_batch_min :  6.124054925749078e-05
reward/total_batch :  0.0012861981435235874
time/iteration_time :  115.52590799331665
time/fps :  2337.138090406698
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.5s | FPS: 2337
Policy Loss: -0.0010, Value Loss: 0.1172, Entropy: 2.4464
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.013, max=0.092, sum=868.0
Extrinsic raw: Œº=0.0014592592592592593

=== Iteration 22/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.99s
EPOCH 1 took 18.45s
update_step :  22
reward/intrinsic_batch_mean :  0.0012522958891436078
reward/extrinsic_batch_mean :  0.0011925925925925925
loss/policy :  -0.0018980929903178053
loss/rnd :  0.00011865718569782929
loss/value :  0.08328051492571831
loss/value_i :  0.00020007476026269919
loss/value_e :  0.08308044019522089
loss/entropy :  2.447115587465691
reward/intrinsic_running :  0.0332868792177175
reward/extrinsic_running :  0.0011925925925925925
reward/intrinsic_std_running :  0.09785007316591833
reward/extrinsic_std_running :  0.1157682228705779
reward/intrinsic_batch_std :  0.0010076346868638938
reward/intrinsic_batch_max :  0.009340970776975155
reward/intrinsic_batch_min :  9.708311699796468e-05
reward/total_batch :  0.0012522958891436078
time/iteration_time :  115.24165487289429
time/fps :  2342.9028357653865
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.2s | FPS: 2343
Policy Loss: -0.0019, Value Loss: 0.0833, Entropy: 2.4471
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.013, max=0.095, sum=863.9
Extrinsic raw: Œº=0.0011925925925925925

=== Iteration 23/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.93s
EPOCH 1 took 19.56s
update_step :  23
reward/intrinsic_batch_mean :  0.0014342952457594004
reward/extrinsic_batch_mean :  0.0012444444444444445
loss/policy :  -0.0012866602452104967
loss/rnd :  0.0001313199765828551
loss/value :  0.08965227171552903
loss/value_i :  0.00024280935163419187
loss/value_e :  0.08940946217626333
loss/entropy :  2.4379178538466943
reward/intrinsic_running :  0.03237717013634168
reward/extrinsic_running :  0.0012444444444444445
reward/intrinsic_std_running :  0.09580941233538245
reward/extrinsic_std_running :  0.12219568196690922
reward/intrinsic_batch_std :  0.0011503340225258684
reward/intrinsic_batch_max :  0.009107894264161587
reward/intrinsic_batch_min :  0.00012076687562512234
reward/total_batch :  0.0014342952457594004
time/iteration_time :  117.42921614646912
time/fps :  2299.257449383208
data/episodes_collected :  60
data/frames_collected :  270000
Timer 117.4s | FPS: 2299
Policy Loss: -0.0013, Value Loss: 0.0897, Entropy: 2.4379
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.015, max=0.095, sum=1010.5
Extrinsic raw: Œº=0.0012444444444444445

=== Iteration 24/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.43s
EPOCH 1 took 19.20s
update_step :  24
reward/intrinsic_batch_mean :  0.0014039505039016215
reward/extrinsic_batch_mean :  0.0004222222222222222
loss/policy :  -0.001599382102898689
loss/rnd :  0.00012621363059610525
loss/value :  0.04178154786032709
loss/value_i :  0.00025021851703940865
loss/value_e :  0.04153132926898472
loss/entropy :  2.431401064901641
reward/intrinsic_running :  0.03153068566628526
reward/extrinsic_running :  0.0004222222222222222
reward/intrinsic_std_running :  0.09389616005203766
reward/extrinsic_std_running :  0.07753020598132172
reward/intrinsic_batch_std :  0.0011659576850503828
reward/intrinsic_batch_max :  0.01108519360423088
reward/intrinsic_batch_min :  0.00011256045399932191
reward/total_batch :  0.0014039505039016215
time/iteration_time :  113.87294363975525
time/fps :  2371.063672984192
data/episodes_collected :  60
data/frames_collected :  270000
Timer 113.9s | FPS: 2371
Policy Loss: -0.0016, Value Loss: 0.0418, Entropy: 2.4314
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.015, max=0.118, sum=1009.3
Extrinsic raw: Œº=0.0004222222222222222

=== Iteration 25/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.83s
EPOCH 1 took 18.61s
update_step :  25
reward/intrinsic_batch_mean :  0.0016908295270223687
reward/extrinsic_batch_mean :  0.0005851851851851852
loss/policy :  -0.0006530151621325675
loss/rnd :  0.00014769030707556698
loss/value :  0.05014957728880373
loss/value_i :  0.00030168406043561515
loss/value_e :  0.0498478931064407
loss/entropy :  2.4085455157540063
reward/intrinsic_running :  0.030833598360343356
reward/extrinsic_running :  0.0005851851851851852
reward/intrinsic_std_running :  0.09208046245957308
reward/extrinsic_std_running :  0.08651498565846394
reward/intrinsic_batch_std :  0.0012545218574418911
reward/intrinsic_batch_max :  0.009471769444644451
reward/intrinsic_batch_min :  8.538048132322729e-05
reward/total_batch :  0.0016908295270223687
time/iteration_time :  114.50998759269714
time/fps :  2357.8729303540613
data/episodes_collected :  60
data/frames_collected :  270000
Timer 114.5s | FPS: 2358
Policy Loss: -0.0007, Value Loss: 0.0501, Entropy: 2.4085
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.018, max=0.103, sum=1239.5
Extrinsic raw: Œº=0.0005851851851851852

=== Iteration 26/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.51s
EPOCH 1 took 19.16s
update_step :  26
reward/intrinsic_batch_mean :  0.0018365634732066948
reward/extrinsic_batch_mean :  -1.4814814814814815e-05
loss/policy :  -0.0008920904488838984
loss/rnd :  0.00015475347467916876
loss/value :  0.013600167719590845
loss/value_i :  0.0003085707752633783
loss/value_e :  0.013291596978281936
loss/entropy :  2.399852474530538
reward/intrinsic_running :  0.03022091413738798
reward/extrinsic_running :  -1.4814814814814815e-05
reward/intrinsic_std_running :  0.0903624701493204
reward/extrinsic_std_running :  0.039534323612327345
reward/intrinsic_batch_std :  0.0012599259419948596
reward/intrinsic_batch_max :  0.010025732219219208
reward/intrinsic_batch_min :  0.00012522496399469674
reward/total_batch :  0.0018365634732066948
time/iteration_time :  116.30063438415527
time/fps :  2321.569451703564
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.3s | FPS: 2322
Policy Loss: -0.0009, Value Loss: 0.0136, Entropy: 2.3999
RND Loss: 0.0002
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.020, max=0.111, sum=1371.9
Extrinsic raw: Œº=-1.4814814814814815e-05

=== Iteration 27/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.22s
EPOCH 1 took 18.85s
update_step :  27
reward/intrinsic_batch_mean :  0.00173349767092624
reward/extrinsic_batch_mean :  0.00015555555555555556
loss/policy :  -0.0011336102051603975
loss/rnd :  0.00014486997833680078
loss/value :  0.02448243341606223
loss/value_i :  0.0002750714526028429
loss/value_e :  0.024207361917378323
loss/entropy :  2.3919684742436265
reward/intrinsic_running :  0.029614539517887967
reward/extrinsic_running :  0.00015555555555555556
reward/intrinsic_std_running :  0.08874084120635295
reward/extrinsic_std_running :  0.0550754857972444
reward/intrinsic_batch_std :  0.001141630287148759
reward/intrinsic_batch_max :  0.00878237932920456
reward/intrinsic_batch_min :  0.00011138775880681351
reward/total_batch :  0.00173349767092624
time/iteration_time :  114.36046743392944
time/fps :  2360.955722360873
data/episodes_collected :  60
data/frames_collected :  270000
Timer 114.4s | FPS: 2361
Policy Loss: -0.0011, Value Loss: 0.0245, Entropy: 2.3920
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.020, max=0.099, sum=1318.6
Extrinsic raw: Œº=0.00015555555555555556

=== Iteration 28/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.99s
EPOCH 1 took 18.59s
update_step :  28
reward/intrinsic_batch_mean :  0.0016325531207246025
reward/extrinsic_batch_mean :  0.00015555555555555556
loss/policy :  -0.0009231973434163427
loss/rnd :  0.0001351771612517592
loss/value :  0.01982026990982845
loss/value_i :  0.00026159805058727437
loss/value_e :  0.01955867183366508
loss/entropy :  2.395394299969529
reward/intrinsic_running :  0.02901273608836006
reward/extrinsic_running :  0.00015555555555555556
reward/intrinsic_std_running :  0.0872094854898248
reward/extrinsic_std_running :  0.05507548579724441
reward/intrinsic_batch_std :  0.0010593165430328247
reward/intrinsic_batch_max :  0.007877478376030922
reward/intrinsic_batch_min :  0.00010559001384535804
reward/total_batch :  0.0016325531207246025
time/iteration_time :  115.27596759796143
time/fps :  2342.2054537998497
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.3s | FPS: 2342
Policy Loss: -0.0009, Value Loss: 0.0198, Entropy: 2.3954
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.019, max=0.090, sum=1263.6
Extrinsic raw: Œº=0.00015555555555555556

=== Iteration 29/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.32s
EPOCH 1 took 18.67s
update_step :  29
reward/intrinsic_batch_mean :  0.0017293166726144221
reward/extrinsic_batch_mean :  -0.00015555555555555556
loss/policy :  -0.0007308638869841216
loss/rnd :  0.0001405330540640562
loss/value :  0.004924579047259282
loss/value_i :  0.0002654435339729499
loss/value_e :  0.004659135522546641
loss/entropy :  2.3808251438718853
reward/intrinsic_running :  0.028475630271610406
reward/extrinsic_running :  -0.00015555555555555556
reward/intrinsic_std_running :  0.0857501445691226
reward/extrinsic_std_running :  0.008817799059114104
reward/intrinsic_batch_std :  0.00103260657964397
reward/intrinsic_batch_max :  0.0073703741654753685
reward/intrinsic_batch_min :  0.0001380244648316875
reward/total_batch :  0.0017293166726144221
time/iteration_time :  113.80161428451538
time/fps :  2372.5498245127974
data/episodes_collected :  60
data/frames_collected :  270000
Timer 113.8s | FPS: 2373
Policy Loss: -0.0007, Value Loss: 0.0049, Entropy: 2.3808
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.020, max=0.086, sum=1361.3
Extrinsic raw: Œº=-0.00015555555555555556

=== Iteration 30/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.73s
EPOCH 1 took 19.61s
update_step :  30
reward/intrinsic_batch_mean :  0.0016567663554019182
reward/extrinsic_batch_mean :  -0.00014074074074074073
loss/policy :  -0.001187833048981812
loss/rnd :  0.0001328346369325917
loss/value :  0.00421590196801293
loss/value_i :  0.0002462163289292772
loss/value_e :  0.0039696856437138085
loss/entropy :  2.374045462319345
reward/intrinsic_running :  0.02794658233770108
reward/extrinsic_running :  -0.00014074074074074073
reward/intrinsic_std_running :  0.08436533326319398
reward/extrinsic_std_running :  0.008387524212439932
reward/intrinsic_batch_std :  0.0009493787023760526
reward/intrinsic_batch_max :  0.007124061230570078
reward/intrinsic_batch_min :  0.00011136382090626284
reward/total_batch :  0.0016567663554019182
time/iteration_time :  116.34234261512756
time/fps :  2320.737179009604
data/episodes_collected :  60
data/frames_collected :  270000
Timer 116.3s | FPS: 2321
Policy Loss: -0.0012, Value Loss: 0.0042, Entropy: 2.3740
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.020, max=0.084, sum=1325.6
Extrinsic raw: Œº=-0.00014074074074074073

=== Iteration 31/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 19.26s
EPOCH 1 took 18.65s
update_step :  31
reward/intrinsic_batch_mean :  0.0016181031055199802
reward/extrinsic_batch_mean :  0.00016296296296296295
loss/policy :  -0.0003558747632654779
loss/rnd :  0.00012884338546868867
loss/value :  0.024556527182346945
loss/value_i :  0.00025777423632627523
loss/value_e :  0.02429875287678883
loss/entropy :  2.3809651136398315
reward/intrinsic_running :  0.027431996037139614
reward/extrinsic_running :  0.00016296296296296295
reward/intrinsic_std_running :  0.0830489922202352
reward/extrinsic_std_running :  0.05504183020850899
reward/intrinsic_batch_std :  0.0009060163552437105
reward/intrinsic_batch_max :  0.007331874221563339
reward/intrinsic_batch_min :  9.719775698613375e-05
reward/total_batch :  0.0016181031055199802
time/iteration_time :  117.03897762298584
time/fps :  2306.9237743151084
data/episodes_collected :  60
data/frames_collected :  270000
Timer 117.0s | FPS: 2307
Policy Loss: -0.0004, Value Loss: 0.0246, Entropy: 2.3810
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.019, max=0.088, sum=1315.2
Extrinsic raw: Œº=0.00016296296296296295

=== Iteration 32/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.22s
EPOCH 1 took 19.00s
update_step :  32
reward/intrinsic_batch_mean :  0.0016596748480077453
reward/extrinsic_batch_mean :  -0.00014074074074074073
loss/policy :  -0.0003227700777422411
loss/rnd :  0.00013023818985058108
loss/value :  0.003886573859772673
loss/value_i :  0.00023980676258250958
loss/value_e :  0.003646767112624013
loss/entropy :  2.3708917155410303
reward/intrinsic_running :  0.026957848776422377
reward/extrinsic_running :  -0.00014074074074074073
reward/intrinsic_std_running :  0.08179035318484379
reward/extrinsic_std_running :  0.008387524212439932
reward/intrinsic_batch_std :  0.0008733304543221647
reward/intrinsic_batch_max :  0.007018116302788258
reward/intrinsic_batch_min :  0.00012219900963827968
reward/total_batch :  0.0016596748480077453
time/iteration_time :  115.94346737861633
time/fps :  2328.7211095585762
data/episodes_collected :  60
data/frames_collected :  270000
Timer 115.9s | FPS: 2329
Policy Loss: -0.0003, Value Loss: 0.0039, Entropy: 2.3709
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.020, max=0.086, sum=1369.7
Extrinsic raw: Œº=-0.00014074074074074073

=== Iteration 33/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.41s
EPOCH 1 took 18.86s
update_step :  33
reward/intrinsic_batch_mean :  0.0014926596661490664
reward/extrinsic_batch_mean :  -0.00016296296296296295
loss/policy :  -0.00117529960257481
loss/rnd :  0.00011662149435406636
loss/value :  0.003507562563754618
loss/value_i :  0.00023740058157162628
loss/value_e :  0.003270161990781851
loss/entropy :  2.378376516428861
reward/intrinsic_running :  0.0264695851670824
reward/extrinsic_running :  -0.00016296296296296295
reward/intrinsic_std_running :  0.08059458579588792
reward/extrinsic_std_running :  0.009025238199304427
reward/intrinsic_batch_std :  0.0008483949468363939
reward/intrinsic_batch_max :  0.00680002523586154
reward/intrinsic_batch_min :  0.00011715554865077138
reward/total_batch :  0.0014926596661490664
time/iteration_time :  113.91220664978027
time/fps :  2370.246419947838
data/episodes_collected :  60
data/frames_collected :  270000
Timer 113.9s | FPS: 2370
Policy Loss: -0.0012, Value Loss: 0.0035, Entropy: 2.3784
RND Loss: 0.0001
--------------------------------
Iteration Rollout Rewards:
Intrinsic scaled: Œº=0.019, max=0.084, sum=1250.1
Extrinsic raw: Œº=-0.00016296296296296295

=== Iteration 34/1000 ===
Collected 60 episodes, 270000 frames
EPOCH 0 took 18.85s
EPOCH 1 took 18.54s
update_step :  34
reward/intrinsic_batch_mean :  0.0015023723229071711
reward/extrinsic_batch_mean :  -0.00018518518518518518
loss/policy :  -0.0005969793536209953
loss/rnd :  0.00011592070509460896
loss/value :  0.003180839477643145
loss/value_i :  0.0002092479884264652
loss/value_e 